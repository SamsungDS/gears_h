n_epochs: 2000
callbacks:
- name: csv

data:
  directory: slhmodels
  experiment: slh
  data_path: <PATH>
  n_train: 100
  n_valid: 10
  bond_fraction : 1.0
  sampling_alpha: 0.0
  batch_size: 2
  valid_batch_size: 2
  n_cpus: 4
  atoms_pad_multiple: 50 
  nl_pad_multiple: 5000 

model:
  atom_centered: 
    descriptor:
      descriptor_name: ShallowTDSAAtomCenteredDescriptor
      use_fused_tensor: False
      num_tensordenses: 2
      max_tensordense_degree: 4
      num_tensordense_features: 12
      mp_steps: 0
      mp_degree: 4
      mp_options:
        num_heads: 4
        qkv_features: 32
      mp_basis_options:
        cutoff_fn: smooth_cutoff
        radial_fn: basic_fourier
        radial_kwargs: {}
        max_degree: 2
        num: 8
    radial_basis:
      cutoff: <CUTOFF>
      num_radial: 16
      max_degree: 2
      num_elemental_embedding: 32
  bond_centered:
    cutoff: <CUTOFF>
    max_basis_degree: 2
    max_degree: 4
    tensor_module: tensor
    tensor_module_dtype: float32
    bond_expansion_options:
      cutoff_fn: smooth_cutoff
      radial_fn: basic_fourier
      radial_kwargs: {}
      max_degree: 2
      num: 8
  mlp:
    mlp_layer_widths: [32,16,32]
    mlp_dtype: float32
    mlp_activation_function: bent_identity

optimizer:
  name: "adam"
  lr: 0.001
  opt_kwargs: {nesterov: True}
  schedule:
    name: exponential_decay
    decay_rate: 0.9999
    end_value: 0.0001
    transition_begin: 20
    transition_steps: 1

loss:
  # We do normed error of feature, and a combination of RMSE and MSE
  # seems to work well
  name: weighted_mse_and_rmse
  loss_parameters:
    # These are relative weights for on and off diagonal losses
    # These settings are acceptable, but not necessarily optimal
    off_diagonal_weight: 4.0
    on_diagonal_weight : 1.0
    mse_weight         : 1.0
    rmse_weight        : 1.0
    # Multiple the total loss by this number, we think it's nicer for floating point
    # but we're not entirely sure strictly speaking
    loss_multiplier    : 5.0

# If the loss doesn't improve for patience epochs, stop training
patience: null
# Seed for all RNGs in the workflow
seed: 2465
# Whether to disable progress bar during training or not 
disable_pbar: False
